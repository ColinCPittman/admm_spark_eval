import org.apache.spark.sql.SparkSession
import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.util.MLUtils
import org.apache.spark.storage.StorageLevel

import breeze.linalg.{DenseVector, norm}
import breeze.numerics.sqrt
import breeze.optimize.{DiffFunction, LBFGS}

import java.io.PrintWriter

/**
 * ============================================================================================
 * 1. DATA STRUCTURES & INTERFACES (as per UML Diagram)
 * ============================================================================================
 */

/**
 * Represents the state for a single data partition in the ADMM algorithm.
 * As described in the paper, each partition `j` maintains its own local model `w_j` and
 * a scaled dual variable `u_j`.
 *
 * @param localWeightVector_w  The local model parameters `w` for this partition.
 * @param scaledDualVariable_u The scaled dual variable `u` for this partition.
 */
case class ADMMState(
    localWeightVector_w: DenseVector[Double],
    scaledDualVariable_u: DenseVector[Double]
)

/**
 * A trait (interface) for the ADMM update logic, as shown in the UML diagram.
 * This separates the core mathematical operations from the orchestration logic, allowing
 * different optimization problems (e.g., Lasso, SVM) to be implemented by creating new
 * classes that extend this trait.
 */
trait ADMMUpdater extends Serializable {

    /** Solves the subproblem for the local weight vector `w` on a single partition (Equation 3). */
    def wUpdate(
        partitionData: Iterator[(Double, DenseVector[Double])],
        currentState: ADMMState,
        globalConsensusVariable_z: DenseVector[Double],
        penaltyParameter_rho: Double
    ): ADMMState

    /** Updates the global consensus variable `z` on the driver (Equation 4). */
    def zUpdate(
        averageLocalWeights_w_avg: DenseVector[Double],
        averageDualVariable_u_avg: DenseVector[Double],
        penaltyParameter_rho: Double,
        numPartitions: Long
    ): DenseVector[Double]

    /** Updates the scaled dual variable `u` on a single partition (Equation 5). */
    def uUpdate(
        currentState: ADMMState,
        globalConsensusVariable_z: DenseVector[Double]
    ): ADMMState

    /** Determines if the algorithm has converged by checking residuals against tolerances (Equation 7). */
    def checkConvergence(
        primalResidual: Double,
        dualResidual: Double,
        normOfAllLocalWeights: Double,
        normOfGlobalZ: Double,
        numPartitions: Long,
        numFeatures: Int
    ): Boolean

    /** Implements the adaptive rho update scheme to improve convergence speed (Equation 8). */
    def updateRho(
        currentRho: Double,
        primalResidual: Double,
        dualResidual: Double
    ): Double
}


/**
 * ============================================================================================
 * 2. CONCRETE IMPLEMENTATION FOR L2-REGULARIZED LOGISTIC REGRESSION
 * ============================================================================================
 */

/**
 * Implements the ADMMUpdater trait for L2-Regularized Logistic Regression (SquaredL2Updater in the diagram).
 * This class contains the specific mathematical formulas for the problem.
 *
 * @param regularizationParam_lambda The L2 regularization parameter, Œª.
 */
class SquaredL2Updater(val regularizationParam_lambda: Double) extends ADMMUpdater {

    /**
     * ULTRA MEMORY-OPTIMIZED w-update that processes data streaming without creating intermediate vectors.
     * This avoids OutOfMemoryError by never loading entire partitions and reusing gradient accumulators.
     */
    override def wUpdate(
        partitionData: Iterator[(Double, DenseVector[Double])],
        currentState: ADMMState,
        globalConsensusVariable_z: DenseVector[Double],
        penaltyParameter_rho: Double
    ): ADMMState = {

        // Convert iterator to array to enable multiple passes (needed for L-BFGS)
        // Process in ultra-small chunks to minimize memory
        val dataBuffer = scala.collection.mutable.ArrayBuffer[(Double, DenseVector[Double])]()
        val maxSamples = 20000  // Ultra-conservative limit per partition
        var count = 0
        
        while (partitionData.hasNext && count < maxSamples) {
            dataBuffer += partitionData.next()
            count += 1
            
            // Force GC every 1000 samples
            if (count % 1000 == 0) {
                System.gc()
            }
        }
        
        if (dataBuffer.isEmpty) return currentState
        println(s"Ultra-memory processing ${dataBuffer.size} samples in partition")

        val costFunction = new DiffFunction[DenseVector[Double]] {
            def calculate(w: DenseVector[Double]): (Double, DenseVector[Double]) = {
                var totalLoss = 0.0
                val gradient = DenseVector.zeros[Double](w.length)  // Reuse this accumulator
                val miniSize = 100  // Ultra-small batches
                
                // Process in tiny streaming batches
                for (start <- dataBuffer.indices by miniSize) {
                    val end = math.min(start + miniSize, dataBuffer.size)
                    var batchLoss = 0.0
                    
                    // Accumulate gradient in-place to avoid creating intermediate vectors
                    for (i <- start until end) {
                        val (label, features) = dataBuffer(i)
                        val prediction = features.t * w
                        val expTerm = math.exp(-label * prediction)
                        
                        // Logistic loss
                        batchLoss += math.log(1 + expTerm)
                        
                        // Gradient: accumulate directly into gradient vector
                        val gradientCoeff = -label / (1 + math.exp(label * prediction))
                        var j = 0
                        while (j < features.length) {
                            gradient(j) += features(j) * gradientCoeff
                            j += 1
                        }
                    }
                    
                    totalLoss += batchLoss
                    
                    // Force GC every few mini-batches
                    if (start % (miniSize * 5) == 0) {
                        System.gc()
                    }
                }
                
                // Normalize by number of samples
                totalLoss = totalLoss / dataBuffer.size
                gradient :/= dataBuffer.size.toDouble  // In-place division

                // Augmented Lagrangian term
                val lagrangianResidual = w - globalConsensusVariable_z + currentState.scaledDualVariable_u
                val augmentedLagrangian = (penaltyParameter_rho / 2.0) * (lagrangianResidual.t * lagrangianResidual)
                val lagrangianGradient = lagrangianResidual * penaltyParameter_rho

                // Combine objective and gradient
                (totalLoss + augmentedLagrangian, gradient + lagrangianGradient)
            }
        }

        // Use more conservative L-BFGS settings
        val lbfgs = new LBFGS[DenseVector[Double]](maxIter = 50, m = 5, tolerance = 1e-4)
        val new_w = lbfgs.minimize(costFunction, currentState.localWeightVector_w)

        currentState.copy(localWeightVector_w = new_w)
    }

    /**
     * Performs the z-update on the driver.
     * This solves Equation (4) from the paper:
     *   z^(k+1) = (N*œÅ / (N*œÅ + 2*Œª)) * (w_avg^(k+1) + u_avg^k)
     */
    override def zUpdate(
        averageLocalWeights_w_avg: DenseVector[Double],
        averageDualVariable_u_avg: DenseVector[Double],
        penaltyParameter_rho: Double,
        numPartitions: Long
    ): DenseVector[Double] = {
        val N = numPartitions.toDouble
        val numerator = N * penaltyParameter_rho
        val denominator = numerator + (2 * regularizationParam_lambda)
        val shrinkageFactor = numerator / denominator

        (averageLocalWeights_w_avg + averageDualVariable_u_avg) * shrinkageFactor
    }

    /**
     * Performs the u-update on each partition.
     * This performs Equation (5) from the paper:
     *   u^(k+1) = u^k + w^(k+1) - z^(k+1)
     */
    override def uUpdate(
        currentState: ADMMState,
        globalConsensusVariable_z: DenseVector[Double]
    ): ADMMState = {
        val new_u = currentState.scaledDualVariable_u + currentState.localWeightVector_w - globalConsensusVariable_z
        currentState.copy(scaledDualVariable_u = new_u)
    }

    /**
     * Checks for convergence based on primal and dual residuals.
     * Implements the stopping criterion from Equation (7) in the paper:
     *   ||r^k|| <= ùúñùëùùëüùëñ = sqrt(N*p)*ùúñ_abs + ùúñ_rel * max{ ||w||, ||z|| }
     * where ||w|| is sqrt(Œ£ ||w_i||¬≤).
     * The dual residual stopping condition is analogous.
     */
    override def checkConvergence(
        primalResidual: Double,
        dualResidual: Double,
        normOfAllLocalWeights: Double, // This is sqrt(Œ£ ||w_i||¬≤)
        normOfGlobalZ: Double,
        numPartitions: Long,
        numFeatures: Int
    ): Boolean = {
        // Use Su's paper tolerances (Section 3.1): Œµ_abs = Œµ_rel = 10^-3
        val epsilon_absolute = 1e-3
        val epsilon_relative = 1e-3

        val N = numPartitions.toDouble
        
        // Su's paper Equation (7): Œµ_pri = ‚àöN¬∑Œµ_abs + Œµ_rel¬∑max{‚àö‚àë‚Äñw_i‚Äñ‚ÇÇ¬≤, ‚Äñz‚Äñ‚ÇÇ}
        val primalTolerance = math.sqrt(N) * epsilon_absolute + epsilon_relative * math.max(normOfAllLocalWeights, normOfGlobalZ)

        println(f"Primal Residual: $primalResidual%.6f, Primal Tolerance: $primalTolerance%.6f")
        println(f"Dual Residual:   $dualResidual%.6f (for monitoring only)")

        // Su's paper only checks primal residual (Equation 7)
        primalResidual <= primalTolerance
    }

    /**
     * Automatically updates the penalty parameter `œÅ` based on the ratio of primal and dual residuals.
     * Implements Equation (8) from paper section 3.4.
     */
    override def updateRho(
        currentRho: Double,
        primalResidual: Double,
        dualResidual: Double
    ): Double = {
        val mu = 10.0      // Balance parameter (recommended value)
        val tau_incr = 2.0 // Increment factor (recommended value)
        val tau_decr = 2.0 // Decrement factor (recommended value)

        if (primalResidual > mu * dualResidual) {
            currentRho * tau_incr
        } else if (dualResidual > mu * primalResidual) {
            currentRho / tau_decr
        } else {
            currentRho
        }
    }
}

/**
 * ============================================================================================
 * 3. ORCHESTRATION AND EXECUTION LOGIC
 * ============================================================================================
 */

/**
 * Orchestrates the distributed ADMM optimization process, as described in the paper.
 * This class corresponds to the `ADMMOptimizer` in the UML diagram. It manages the main
 * iteration loop, data persistence, and communication (broadcasts and aggregations).
 *
 * @param updater An instance of ADMMUpdater containing the problem-specific math.
 * @param numPartitions The number of partitions `N` to split the data into.
 * @param numFeatures The number of features `p` in the dataset.
 */
class ADMMOptimizer(

    val updater: ADMMUpdater,
    val numPartitions: Int,
    val numFeatures: Int) extends Serializable {

    def run(data: RDD[(Double, DenseVector[Double])], maxIterations: Int): DenseVector[Double] = {
        val sparkContext = data.sparkContext

        val partitionedData = data.repartition(numPartitions).persist(StorageLevel.DISK_ONLY)
        val numActualPartitions = partitionedData.getNumPartitions

        // Initialize ADMMState on each partition with random vectors for w and zero for u.
        var statesRDD = partitionedData.mapPartitions(_ => Iterator(ADMMState(
            localWeightVector_w = DenseVector.rand[Double](numFeatures) * 0.01,
            scaledDualVariable_u = DenseVector.zeros[Double](numFeatures)
        )), preservesPartitioning = true).persist(StorageLevel.DISK_ONLY)

        // Initialize global variables on the driver
        var globalConsensusVariable_z = DenseVector.zeros[Double](numFeatures)
        var previousGlobalZ = DenseVector.zeros[Double](numFeatures)
        var penaltyParameter_rho = 1.0

        var isConverged = false
        var iteration = 0
        // Enforce a minimum number of iterations before we allow convergence.
        val minIterationsRequired = 10

        while (!isConverged && iteration < maxIterations) {
            iteration += 1
            println(s"\n--- Iteration: $iteration ---")

            // Broadcast global variables to all executors
            val z_broadcast = sparkContext.broadcast(globalConsensusVariable_z)
            val rho_broadcast = sparkContext.broadcast(penaltyParameter_rho)

            // Combine data and state RDDs for processing
            val dataAndStatesRDD = partitionedData.zipPartitions(statesRDD, preservesPartitioning = true) {
                (dataIter, stateIter) => Iterator((dataIter, stateIter.next))
            }

            // Step 1: w-update (in parallel on each partition)
            val states_after_w_update = dataAndStatesRDD.map { case (partitionData, state) =>
                updater.wUpdate(partitionData, state, z_broadcast.value, rho_broadcast.value)
            }.persist(StorageLevel.DISK_ONLY)

            // Step 2: Aggregate results for z-update
            // We use treeAggregate for efficiency, as recommended in the paper.
            // We compute sum(w), sum(u), and sum(||w||¬≤) in a single pass.
            val (w_sum, u_sum, w_squared_norm_sum) = states_after_w_update
              .map(s => (s.localWeightVector_w, s.scaledDualVariable_u))
              .treeAggregate(
                (DenseVector.zeros[Double](numFeatures), DenseVector.zeros[Double](numFeatures), 0.0) // Zero value
              )(
                seqOp = (agg, wu_tuple) => {
                    val (w, u) = wu_tuple
                    val w_norm_sq = norm(w, 2) * norm(w, 2)
                    (agg._1 + w, agg._2 + u, agg._3 + w_norm_sq)
                },
                combOp = (agg1, agg2) => {
                    (agg1._1 + agg2._1, agg1._2 + agg2._2, agg1._3 + agg2._3)
                }
              )

            val w_avg = w_sum / numActualPartitions.toDouble
            val u_avg = u_sum / numActualPartitions.toDouble

            // Step 3: z-update (on driver)
            previousGlobalZ = globalConsensusVariable_z
            globalConsensusVariable_z = updater.zUpdate(w_avg, u_avg, penaltyParameter_rho, numActualPartitions)
            val new_z_broadcast = sparkContext.broadcast(globalConsensusVariable_z)

            // Step 4: u-update (in parallel on each partition)
            val states_after_u_update = states_after_w_update.map { state =>
                updater.uUpdate(state, new_z_broadcast.value)
            }.persist(StorageLevel.DISK_ONLY)

            // Step 5: Check for convergence
            // Primal residual r^k = sqrt( Œ£ ||w_i - z||¬≤ )
            val primalResidual_squared_sum = states_after_u_update.map(s => norm(s.localWeightVector_w - globalConsensusVariable_z, 2)).map(n => n * n).sum()
            val primalResidual = math.sqrt(primalResidual_squared_sum)
            
            // Dual residual s^k = œÅ * sqrt(N) * ||z^k - z^(k-1)||  (consensus form)
            val dualResidual = penaltyParameter_rho * math.sqrt(numActualPartitions) * norm(globalConsensusVariable_z - previousGlobalZ, 2)

            val normOfAllLocalWeights = math.sqrt(w_squared_norm_sum)
            val normOfGlobalZ = norm(globalConsensusVariable_z, 2)

            isConverged = updater.checkConvergence(primalResidual, dualResidual, normOfAllLocalWeights, normOfGlobalZ, numActualPartitions, numFeatures)

            // Prevent premature convergence in very early iterations
            if (iteration < minIterationsRequired) {
                isConverged = false
            }

            // Step 6: Update rho for the next iteration
            penaltyParameter_rho = updater.updateRho(penaltyParameter_rho, primalResidual, dualResidual)
            println(s"New Rho: $penaltyParameter_rho")

            // Cleanup for next iteration
            statesRDD.unpersist()
            states_after_w_update.unpersist()
            statesRDD = states_after_u_update // The new state for the next iteration
        }

        partitionedData.unpersist()
        statesRDD.unpersist()

        println(s"\nConvergence reached after $iteration iterations.")
        globalConsensusVariable_z // The final model
    }
}

/**
 * A user-facing class that wraps the ADMM optimizer for logistic regression.
 * This corresponds to the `SparseLogisticRegressionWithADMM` class in the UML diagram,
 * but is named more generically as the implementation uses dense vectors.
 * It inherits from a placeholder `GeneralizedLinearAlgorithm` to match the diagram.
 */
abstract class GeneralizedLinearAlgorithm
class LogisticRegressionWithADMM(
    val regularizationParam_lambda: Double,
    val numPartitions: Int,
    val maxIterations: Int
) extends GeneralizedLinearAlgorithm {

    def train(data: RDD[(Double, DenseVector[Double])]): DenseVector[Double] = {
        require(data.getStorageLevel != StorageLevel.NONE, "Input RDD must be persisted.")
        val numFeatures = data.first()._2.length
        
        println(s"Running ADMM with $numPartitions partitions, $numFeatures features.")
        println(s"Lambda: $regularizationParam_lambda, Max Iterations: $maxIterations")

        val updater = new SquaredL2Updater(regularizationParam_lambda)
        val optimizer = new ADMMOptimizer(updater, numPartitions, numFeatures)
        
        optimizer.run(data, maxIterations)
    }
}


/**
 * ============================================================================================
 * 4. RUNNER OBJECT
 * ============================================================================================
 */

/**
 * A runnable object with a main method to load data, execute the optimizer, and save the results.
 */
object ADMMRunner {
    def main(args: Array[String]): Unit = {
        if (args.length < 4) {
            System.err.println("Usage: ADMMRunner <data_path> <num_partitions> <num_features> <output_path>")
            System.exit(1)
        }

        val dataPath = args(0)
        val numPartitions = args(1).toInt
        val numFeatures = args(2).toInt
        val outputPath = args(3)
        
        // --- Hyperparameters ---
        val lambda = 0.1
        val maxIterations = 50

        val spark = SparkSession.builder.appName("ADMM Logistic Regression").getOrCreate()
        val sc = spark.sparkContext

        println(s"Loading data from: $dataPath")
        val data = MLUtils.loadLibSVMFile(sc, dataPath).map {
            labeledPoint => (labeledPoint.label, DenseVector(labeledPoint.features.toArray))
        }.persist(StorageLevel.DISK_ONLY)
        
        // Instantiate the user-facing algorithm class
        val admmLogisticRegression = new LogisticRegressionWithADMM(
            regularizationParam_lambda = lambda,
            numPartitions = numPartitions,
            maxIterations = maxIterations
        )

        // Train the model
        val finalModelWeights = admmLogisticRegression.train(data)

        println(s"\nFinal model weights (first 20):\n${finalModelWeights.toArray.take(20).mkString(",")}")

        // Save the final model weights to a file
        val writer = new PrintWriter(outputPath)
        try {
            writer.println(finalModelWeights.toArray.mkString(","))
        } finally {
            writer.close()
        }

        println(s"Model saved to $outputPath")
        spark.stop()
    }
}

/**
 * ============================================================================================
 * 5. CONVENIENCE FUNCTIONS FOR SPARK SHELL
 * ============================================================================================
 */

/**
 * Lists all available datasets in the sourced folder.
 */
def listDatasets(): Unit = {
  val sourcedPath = "/workspace/data/sourced"
  println(s"Available datasets in $sourcedPath:")
  println("=" * 50)
  
  try {
    val files = new java.io.File(sourcedPath).listFiles()
      .filter(_.isFile)
      .filter(f => f.getName.endsWith(".dat") || f.getName.endsWith(".csv"))
      .sortBy(_.getName)
    
    files.foreach { file =>
      val sizeInMB = file.length() / (1024 * 1024)
      println(f"  ${file.getName}%-35s (${sizeInMB}%3d MB)")
    }
    println()
    println("Usage: runADMM(\"filename.dat\") - will automatically look in sourced folder")
  } catch {
    case e: Exception => println(s"Error listing files: ${e.getMessage}")
  }
}

/**
 * Enhanced function to run ADMM on any dataset by filename.
 * Automatically looks in /workspace/data/sourced/ folder.
 */
def runADMM(filename: String, 
           numPartitions: Int = 10, 
           lambda: Double = 0.1, 
           maxIterations: Int = 50,
           outputPath: String = ""): DenseVector[Double] = {
  
  // Automatically prepend the sourced folder path if just filename is given
  val dataPath = if (filename.startsWith("/")) {
    filename // Full path provided
  } else {
    s"/workspace/data/sourced/$filename" // Just filename, add sourced path
  }
  
  println(s"Loading data from: $dataPath")
  val data = MLUtils.loadLibSVMFile(sc, dataPath).map {
    lp => (lp.label, DenseVector(lp.features.toArray))
  }.persist(StorageLevel.DISK_ONLY)
  
  // Automatically determine number of features from the data
  val n_features = data.map(_._2.length).max()
  println(s"Detected $n_features features in the dataset")
  
  // Count samples
  val numSamples = data.count()
  println(s"Dataset contains $numSamples samples")
  
  println(s"Running ADMM with:")
  println(s"  - Partitions: $numPartitions")  
  println(s"  - Lambda (regularization): $lambda")
  println(s"  - Max iterations: $maxIterations")
  println(s"  - Features: $n_features")
  
  // Use the proper LogisticRegressionWithADMM class
  val admmLogisticRegression = new LogisticRegressionWithADMM(
    regularizationParam_lambda = lambda,
    numPartitions = numPartitions,
    maxIterations = maxIterations
  )
  
  val startTime = System.currentTimeMillis()
  val final_model = admmLogisticRegression.train(data)
  val endTime = System.currentTimeMillis()
  
  val runTime = (endTime - startTime) / 1000.0
  println(s"\nADMM completed in $runTime seconds")
  println(s"Final model weights (first 10): ${final_model.toArray.take(10).mkString(", ")}")
  
  // Auto-generate output path if not provided
  val finalOutputPath = if (outputPath.isEmpty) {
    val baseFileName = filename.split("/").last.split("\\.").head
    s"/workspace/data/generated/${baseFileName}_admm_model.txt"
  } else {
    outputPath
  }
  
  // Save results
  val writer = new PrintWriter(finalOutputPath)
  writer.println(final_model.toArray.mkString(","))
  writer.close()
  println(s"Model saved to: $finalOutputPath")
  
  data.unpersist()
  final_model
}

/**
 * Interactive function that shows available datasets and prompts for selection.
 */
def runADMM_Interactive(): DenseVector[Double] = {
  listDatasets()
  print("Enter filename to run ADMM on: ")
  val filename = scala.io.StdIn.readLine()
  
  if (filename.trim.isEmpty) {
    println("No filename entered. Using sample_rcv1.dat as default.")
    runADMM("sample_rcv1.dat")
  } else {
    runADMM(filename.trim)
  }
}

/**
 * Quick test function with very few iterations for debugging.
 */
def testADMM(filename: String): DenseVector[Double] = {
  println(s"Running ADMM test on $filename (5 iterations only)...")
  runADMM(filename, numPartitions = 2, lambda = 0.1, maxIterations = 5)
}

/**
 * Function to run ADMM on the full RCV1 training dataset with optimized parameters.
 */
def runADMM_RCV1_Full(numPartitions: Int = 8): DenseVector[Double] = {
  println("Running ADMM on full RCV1 training dataset...")
  println("Note: This will take significantly longer than the sample dataset!")
  runADMM("lyrl2004_tokens_train.dat", numPartitions, lambda = 0.1, maxIterations = 50)
}

/**
 * Function to compare ADMM performance on different RCV1 dataset splits.
 */
def compareRCV1_Datasets(): Unit = {
  val rcv1Files = Array(
    "sample_rcv1.dat",
    "lyrl2004_tokens_train.dat",
    "lyrl2004_tokens_test_pt0.dat"
  )
  
  println("Comparing ADMM performance on different RCV1 datasets:")
  
  for (file <- rcv1Files) {
    println(s"\n=== Testing $file ===")
    try {
      val model = testADMM(file) // Use test version with fewer iterations
      println(s"Completed $file - Model norm: ${norm(model, 2)}")
    } catch {
      case e: Exception => println(s"Error with $file: ${e.getMessage}")
    }
  }
}

/**
 * Helper function to get dataset info without running ADMM.
 */
def datasetInfo(filename: String): Unit = {
  val dataPath = if (filename.startsWith("/")) filename else s"/workspace/data/sourced/$filename"
  
  try {
    println(s"Loading dataset info from: $dataPath")
    val data = MLUtils.loadLibSVMFile(sc, dataPath)
    
    val numSamples = data.count()
    val numFeatures = data.map(_.features.size).max()
    val labels = data.map(_.label).distinct().collect().sorted
    
    println(s"Dataset: $filename")
    println(s"  Samples: $numSamples")
    println(s"  Features: $numFeatures") 
    println(s"  Labels: ${labels.mkString(", ")}")
    
    data.unpersist()
  } catch {
    case e: Exception => println(s"Error reading $filename: ${e.getMessage}")
  }
}

/**
 * ============================================================================================
 * 6. ACCURACY EVALUATION FUNCTIONS (KEY FOR PAPER REPRODUCTION)
 * ============================================================================================
 */

/**
 * Calculate logistic regression prediction accuracy.
 * This is essential for reproducing the paper's claimed 98.17% accuracy.
 */
def calculateAccuracy(testData: RDD[(Double, DenseVector[Double])], model: DenseVector[Double]): Double = {
  val predictions = testData.map { case (actualLabel, features) =>
    // Logistic regression prediction: sigmoid(x^T * w)
    val rawPrediction = features.t * model
    val probability = 1.0 / (1.0 + math.exp(-rawPrediction))
    val predictedLabel = if (probability > 0.5) 1.0 else -1.0
    
    (actualLabel, predictedLabel)
  }
  
  val correct = predictions.filter { case (actual, predicted) => actual == predicted }.count()
  val total = predictions.count()
  
  correct.toDouble / total.toDouble
}

/**
 * Split dataset into training and testing sets.
 * Uses the same 80/20 split methodology as the paper.
 */
def splitTrainTest(data: RDD[(Double, DenseVector[Double])], trainRatio: Double = 0.8): (RDD[(Double, DenseVector[Double])], RDD[(Double, DenseVector[Double])]) = {
  val Array(trainData, testData) = data.randomSplit(Array(trainRatio, 1.0 - trainRatio), seed = 42)
  (trainData.persist(StorageLevel.MEMORY_AND_DISK), testData.persist(StorageLevel.MEMORY_AND_DISK))
}

/**
 * ====================== MEMORY-SAFE ADMM WITH ACCURACY (FIXED) =====================
 */

def runADMM_WithAccuracy(
    filename: String = "rcv1_5percent_sample.dat",  // Now uses proper RCV1 sample with real labels
    sampleRatio: Double = 1.0,     // Use full sample (it's already sampled)
    numPartitions: Int = 8,        // Su used 5-15 partitions  
    lambda: Double = 0.01,         // Smaller lambda like Su likely used
    maxIterations: Int = 50,
    trainRatio: Double = 0.8): Unit = {

  val dataPath = if (filename.startsWith("/")) filename else s"/workspace/data/sourced/$filename"
  println(s"Loading LibSVM data from: $dataPath")
  println("Using Su's paper methodology with proper RCV1 sample (5% subset with real -1/+1 labels)")

  // Load the pre-sampled data (it's already a 5% sample with proper labels)
  val rawData = MLUtils.loadLibSVMFile(sc, dataPath)
  val sampledData = if (sampleRatio < 1.0) {
    rawData.sample(false, sampleRatio, 42)
      .map(lp => (lp.label, DenseVector(lp.features.toArray)))
      .persist(StorageLevel.DISK_ONLY)
  } else {
    rawData.map(lp => (lp.label, DenseVector(lp.features.toArray)))
      .persist(StorageLevel.DISK_ONLY)
  }

  val nSamples = sampledData.count()
  val nFeatures = sampledData.first()._2.length
  println(s"Dataset: $nSamples samples, $nFeatures features (RCV1 5% sample with proper labels)")

  // Use Su's partitioning approach (8 partitions like his experiments)
  val data = sampledData.repartition(numPartitions).persist(StorageLevel.DISK_ONLY)

  // Split train/test
  val Array(train, test) = data.randomSplit(Array(trainRatio, 1.0 - trainRatio), seed = 42)
  train.persist(StorageLevel.DISK_ONLY)
  test.persist(StorageLevel.DISK_ONLY)

  val trainCount = train.count()
  val testCount = test.count()
  println(s"Train: $trainCount samples, Test: $testCount samples")

  // Check label distribution
  val labelCounts = train.map(_._1).countByValue()
  println(s"Training label distribution: ${labelCounts}")

  // Train ADMM using Su's optimized parameters
  val learner = new LogisticRegressionWithADMM(lambda, numPartitions, maxIterations)
  
  println("\n" + "="*60)
  println("STARTING ADMM TRAINING (Su's Paper Methodology with Real Labels)")
  println("="*60)
  
  val t0 = System.currentTimeMillis()
  val model = learner.train(train)
  val t1 = System.currentTimeMillis()
  val runtimeSec = (t1 - t0) / 1000.0

  // Evaluate accuracy
  val accuracy = calculateAccuracy(test, model) * 100.0

  // Debug: Check if model is learning properly
  val modelNorm = norm(model, 2)
  val nonZeroWeights = model.toArray.count(math.abs(_) > 1e-6)
  println(s"\nModel Analysis:")
  println(s"Model L2 norm: $modelNorm")
  println(s"Non-zero weights: $nonZeroWeights / ${model.length}")
  
  // Debug: Check a few predictions manually
  val samplePredictions = test.take(5).map { case (actualLabel, features) =>
    val rawPrediction = features.t * model
    val probability = 1.0 / (1.0 + math.exp(-rawPrediction))
    val predictedLabel = if (probability > 0.5) 1.0 else -1.0
    (actualLabel, rawPrediction, probability, predictedLabel)
  }
  println(s"\nSample Predictions:")
  samplePredictions.foreach { case (actual, raw, prob, pred) =>
    println(f"Actual: $actual, Raw: $raw%.6f, Prob: $prob%.6f, Predicted: $pred")
  }

  println("\n" + "="*60)
  println("ADMM RESULTS WITH PROPER RCV1 DATA")
  println("="*60)
  println(f"Dataset: $filename (RCV1 5%% sample with real labels)")
  println(f"Samples: $nSamples (Train: $trainCount, Test: $testCount)")
  println(f"Features: $nFeatures")
  println(f"Runtime: $runtimeSec%.1f seconds")
  println(s"Accuracy: ${accuracy.formatted("%.2f")}% (Target: 98.17% from Su's paper)")
  println(s"Parameters: Œª=$lambda, MaxIters=$maxIterations, Partitions=$numPartitions")
  println("="*60)

  // Save model
  val base = filename.split("/").last.split("\\.").head
  val out = s"/workspace/data/generated/${base}_admm_model.txt"
  new PrintWriter(out) { write(model.toArray.mkString(",")); close() }
  println(s"Model saved to: $out")

  // Clean up
  rawData.unpersist()
  sampledData.unpersist()
  data.unpersist()
  train.unpersist()
  test.unpersist()
}

/**
 * ====================== SCALED-DOWN EXPERIMENT FOR MEMORY-CONSTRAINED SYSTEMS =====================
 */

/**
 * Runs ADMM on a scaled-down subset of the RCV1 dataset for systems with memory constraints.
 * This function automatically samples ~1000 examples from the full dataset and runs the complete
 * ADMM experiment with accuracy evaluation, designed to replace runADMM_WithAccuracy for
 * resource-limited environments while maintaining experimental validity.
 */
def runADMM_ScaledDown(filename: String = "lyrl2004_vectors_test_pt0.dat"): Unit = {
  println("\n" + "="*60)
  println("ADMM SCALED-DOWN EXPERIMENT")
  println("="*60)
  println(s"Sampling ~1000 examples from $filename for memory-safe execution...")
  
  val dataPath = if (filename.startsWith("/")) filename else s"/workspace/data/sourced/$filename"
  
  // Load and sample the data (0.5% = ~1000 samples from 199K)
  val fullData = MLUtils.loadLibSVMFile(sc, dataPath)
  val sampledData = fullData.sample(false, 0.005, 42)
    .map(lp => (lp.label, DenseVector(lp.features.toArray)))
    .persist(StorageLevel.DISK_ONLY)
  
  val nSamples = sampledData.count()
  val nFeatures = sampledData.first()._2.length
  println(s"Sampled Dataset: $nSamples samples, $nFeatures features")
  
  // Split train/test (80/20)
  val Array(train, test) = sampledData.randomSplit(Array(0.8, 0.2), seed = 42)
  train.persist(StorageLevel.DISK_ONLY)
  test.persist(StorageLevel.DISK_ONLY)
  
  // Run ADMM with memory-safe configuration
  val learner = new LogisticRegressionWithADMM(
    regularizationParam_lambda = 0.1,
    numPartitions = 2, // Small partitions for small data
    maxIterations = 50
  )
  
  val t0 = System.currentTimeMillis()
  val model = learner.train(train)
  val t1 = System.currentTimeMillis()
  val runtimeSec = (t1 - t0) / 1000.0
  
  // Calculate accuracy
  val accuracy = calculateAccuracy(test, model) * 100.0
  
  println("\n" + "="*60)
  println("SCALED-DOWN ADMM RESULTS")
  println("="*60)
  printf("Dataset Size: %d samples (%.1f%% of original)\n", nSamples, (nSamples / 199328.0) * 100)
  printf("Runtime: %.1f seconds\n", runtimeSec)
  printf("Accuracy: %.2f%%\n", accuracy)
  println(s"Features: $nFeatures, Partitions: 2, Lambda: 0.1")
  println("="*60)
  
  // Save model
  val base = filename.split("/").last.split("\\.").head
  val out = s"/workspace/data/generated/${base}_scaled_down_model.txt"
  new PrintWriter(out) { write(model.toArray.mkString(",")); close() }
  println(s"Model saved to: $out")
  
  // Clean up
  sampledData.unpersist()
  train.unpersist()
  test.unpersist()
}


